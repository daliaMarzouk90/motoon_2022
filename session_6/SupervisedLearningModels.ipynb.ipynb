{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Naive Bayes\n",
    "- Decision Trees\n",
    "- Random Forsts Trees\n",
    "- Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "#### Probability\n",
    "- The probabilty of an event is the likely of this event to be happen\n",
    "- In the discrete domain probability is calculated by dividing the number of possibilities of happening the event by the total count of all possible events\n",
    "- For example the probability of getting an orange from a box contains oranges, and apples is the as following <br>\n",
    "<center><b>P(orange) = Count(orange) / (Count(orange) + Count(Apple))</b><center><br/><br/>\n",
    "<center><img src=\"images/probability.jfif\" width=\"350\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Probability\n",
    "- Joint proability if the probability of happing two events together\n",
    "- For example if we have students who may (success of faile) and may be (male or female)\n",
    "- When we say the probability of the student to be success and being male this is a joint probability<br>\n",
    "- All events are E(su,mlae), E(su, female), E(fa, male), and E(fa, female) <br/>\n",
    "and their proabilities are:<br/> \n",
    "<center><b>P(su,mlae), P(su, female), P(fa, male), and P(fa, female)</b><center/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "- For the same example of having students who may (success of faile) and may be (male or female) we says we have already a number of students who are success, and what is the probability of being female<br/>\n",
    "<center><b>P(female|su) = Count(female,su) / (Count(female,su) + Count(male,su)) = Count(female,su) / Count(su)</center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that:\n",
    "<b>Note that:</b><br>\n",
    "<center>P(female, su) = Count(femle, su) / Count(All samples)</center><br>\n",
    "<center>P(su) = Count(su) / Count(All samples)</center><br>\n",
    "<b>So,</b><center>P(female, su) / P(su) = Count(femle, su) / Count(su)</center><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "<center>\n",
    "P(A|B) = P(A,B) / P(B)<br/><br/>\n",
    "P(B|A) = P(A,B) / P(A) &emsp; --> &emsp;P(A,B) = P(B|A) P(A)<br/><br/>\n",
    "<b>So, </b>\n",
    "P(A|B) = P(A,B) / P(B) = P(B|A) P(A) / P(B)<br/><br/>\n",
    "<b>Then bayes rule is, </b>\n",
    "P(A|B) = P(B|A) P(A) / P(B)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule in Machine Learning\n",
    "<ul>\n",
    "<li><b>P(Class|FeatureVector):</b> Posterior probability</li>\n",
    "<li><b>P(FeatureVector):</b> Prior probability</li>\n",
    "<li><b>P(FeatureVector|Class):</b> Likelihood probability</li>\n",
    "<li><b>P(Class):</b> Class probability</li>\n",
    "</ul>\n",
    "<center><b><br/>Posterior = Likelihood * Class_probability / Prior_Probability </b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "<ul>\n",
    "<li> Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\n",
    "(this is the independence assumption)</li>\n",
    "<li> This assumption makes us calculate P(FeatureVector) as the result of multiplicatio of each feature value independently</li>\n",
    "<li> For example in Iris data set the probability of the a certain feature vector is:<br/><br/>\n",
    "<center><I>P(FeatureVector) = P(sepal_length) * P(sepal_width) * P(petal_length) * P(petal_width)<br/><br/>\n",
    "P(FeatureVector|Class_i) = P(sepal_length|Class_i) * P(sepal_width|Class_i) * P(petal_length|Class_i) * P(petal_width|Class_i)</I></center></li><br/>\n",
    "<li> At the time of prediction we calculate the probability of each class given the feature vector</li>\n",
    "<li> We vote for the class with the heighest probability</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfedb6f9ff969e4dee037b4248a00ef2f6a1627a97de70de2314228a5c58e97f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
